<!DOCTYPE html>
<!-- saved from url=(0017)https://xzhou.me/ -->
<html lang="en">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><!--<base target="_blank">-->
    <base href="." target="_blank">
    <link rel="preconnect" href="https://fonts.gstatic.com/">
    <link rel="stylesheet" type="text/css" data-href="https://fonts.googleapis.com/css?family=Lato">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <meta name="viewport" content="width=device-width">
    <title>Hanqing Zhu's Homepage</title>
    <link rel="icon" href="/images/fubao_head.jpg" type="image/jpg">
    <meta name="description" content="I am a Ph.D. student in Electrical Computer Engineering at The University of Texas at Austin">
    <meta name="next-head-count" content="4">
    <link rel="preload" href="./Homepage_files/fd4c9bf86266e80fae27.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/fd4c9bf86266e80fae27.css" data-n-g="">
    <link rel="preload" href="./Homepage_files/a515be4c2f49419f3769.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/a515be4c2f49419f3769.css" data-n-p=""><noscript
        data-n-css=""></noscript>
    <script defer="" nomodule="" src="./Homepage_files/polyfills-a54b4f32bdc1ef890ddd.js.下载"></script>
    <script src="./Homepage_files/webpack-715970c8028b8d8e1f64.js.下载" defer=""></script>
    <script src="./Homepage_files/framework-64eb7138163e04c228e4.js.下载" defer=""></script>
    <script src="./Homepage_files/main-c94e7f60255631414010.js.下载" defer=""></script>
    <script src="./Homepage_files/_app-3fa27215a3c41987e6d2.js.下载" defer=""></script>
    <script src="./Homepage_files/688-b942890a87f9a08b0e31.js.下载" defer=""></script>
    <script src="./Homepage_files/index-335648998a7bdac0c719.js.下载" defer=""></script>
    <script src="./Homepage_files/_buildManifest.js.下载" defer=""></script>
    <script src="./Homepage_files/_ssgManifest.js.下载" defer=""></script>
    <style data-href="https://fonts.googleapis.com/css?family=Lato">
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wWA.woff) format('woff')
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD
        }
    </style>
</head>

<body>
    <div id="__next">
        <div class="styles_container__1_WuM">
            <section>
                <div class="personal_profile__BdQI4"><img class="personal_portrait__BAkO0"
                        src="./images/hanqing.jpg" alt="pottrait">
                    <div class="personal_profileInfo__1Y4pW">
                        <h2 class="personal_name__1_mHK">Zhiwen ("Aaron") Fan</h2>
                        <h3 class="personal_chineseName__18aOD">樊志文</h3>
                        <h3 class="personal_worksFor__2L0De">University of Texas at Austin</h3>
                        <div class="personal_links__p_eYL"><span><a
                                    href="mailto:zhiwenfan@utexas.edu">Email</a></span>
                                    <span><a href="https://scholar.google.com.hk/citations?user=tdoBO3UAAAAJ&hl=en-us">Google Scholar</a></span>
                                    <span><a href="./Homepage_files/Zhiwen_Fan_UTAustin_CV.pdf">CV</a></span>
                                    <span><a href="./Homepage_files/Zhiwen_Fan_UTAustin_ResearchStatement.pdf">Research Statement</a></span>
                                </div>
                    </div>
                </div>
            </section>
            <section>
                <h2>About Me</h2>
                <div>
                    <p>I am a Ph.D. candidate in Electrical Computer Engineering at The University of Texas at Austin advised by <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Prof. Atlas Wang</a> at <a href="https://vita-group.github.io/">VITA group</a>. <br>
                        I work closely with <a href="https://profiles.stanford.edu/marco-pavone">Prof. Marco Pavone</a> and <a href="https://yuewang.xyz/">Prof. Yue Wang</a> on 3D end-to-end models with robust generalization capabilities; with <a href="https://www.ee.ucla.edu/achuta-kadambi/">Prof. Achuta Kadambi</a> on recovering 3D/4D signals that capture the space-time structure of our world from casually captured data; and with <a href="https://ece.gatech.edu/directory/callie-hao">Prof. Callie Hao</a> on hardware-software co-design.
                        I was the awardees of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america">Qualcomm Innovation Fellowship 2022</a>.
                    </p>
                </div>
            </section>
            <section>
                <h2>Recent News</h2>
                <ul>
                    <li>Our NeurIPS'24 (<a href="https://lightgaussian.github.io/">LightGaussian</a>) is selected as <strong>spotlight</strong> presentation. </li>
                    <li>Our <a href="https://arxiv.org/abs/2212.14849">Symbolic Visual RL</a> was accepted by IEEE Trans. PAMI. </li>
                    <li>Our IROS'24 (<a href="https://arxiv.org/abs/2404.00923">Multi-modal 3DGS SLAM</a>)
                        is selected as <strong>oral pitch finalist</strong> presentation. </li>
                    <li>Our CVPR'24 (<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.pdf">Feature-3DGS</a>) is selected as <strong>highlight</strong> presentation. </li>
                    <li>Our CVPR'23 (<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf">NeuralLift-360</a>) is selected as <strong>highlight</strong> presentation. </li>
                    <li>I was one of the awardees of the <strong><span style="color: darkblue;">Qualcomm Innovation Fellowship</span></strong> (North America) 2022 <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america">(QIF 2022)</a>. Innovation title: "Real-time Visual Processing for Autonomous Driving via Video Transformer with Data-Model-Accelerator Tri-design". </li>
                    <li>We won 3rd place in the <strong><span style="color: darkblue;">University Demo Best Demonstration</span></strong> at the 59th Design Automation Conference <a href="https://www.dac.com/">(DAC 2022)</a>. We demo for a multi-task vision transformer on FPGA. </li>
                    <li>Our CVPR'22 (<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf">CADTransformer</a>) is selected as <strong>oral</strong> presentation. </li>
                    <li>Our paper for CVPR'20 (<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf">Cascade Cost Volume</a>) is selected as <strong>oral</strong> presentation. </li>
                </ul>
                <!-- <div class="styles_showMore__JvZFs"><button>Show More</button></div> -->
            </section>
            <section>
                <h2>Commitment</h2>
                <ul>
                    <!-- <li>I am dedicated to collecting high-quality, accessible online courses in artificial intelligence, machine learning and computer vision. Check the note page: <a -->
                        <!-- href="./Homepage_files/online_classes.html">LINK</a>.</li> -->
                    <li> I dedicate 1 hour weekly (2:00pm-2:30pm, 2:30pm-3:00pm, PST on Friday) to mentor and guide students from underrepresented groups or those in need. Interested? Fill out this <a href="https://forms.gle/fu6RzX9PgSNbw6SMA">form</a>.</li>
                    <li> Seeking motivated graduate and undergraduate students to collaborate. Feel free to reach out if you’re interested in joining exciting research projects.  </li>
                </ul>
                <!-- <div class="styles_showMore__JvZFs"><button>Show More</button></div> -->
            </section>


            <section>
                <h2>Researches and Projects<br class="publication-list_mobileBreak__24vsO"></h2>


                <div class="publication_publication__1Icb_">
                    <div class="publication_image__1EUuC"><img src="./Homepage_files/rs_few_shot_3d.svg"
                        alt="loading...">
                </div>
                    <div class="publication_info__kLRGP">
                        <div class="publication_title__3m6SE">Few-Shot 3D Learning with Geometric and Generative Priors</div>
                        <div class="publication_authors__qkFXc">Creating photorealistic 3D environments often lacks dense scene
                            capture with annotated poses. My research integrates geometric principles with generative priors from
                            large datasets, enabling 3D/4D asset creation from multi-modal inputs by leveraging statistical patterns
                            and combining deterministic geometry with generative models for efficient learning.</div>
                        <div class="publication_links__aEpO_">
                            <ul>
                                <li> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf">CasMVSNet </a>(CVPR'20, Oral), <a href="https://ieeexplore-ieee-org.ezproxy.lib.utexas.edu/abstract/document/10550594/">Cas6D</a> (3DV'24).</li>
                                <li> <a href="https://vita-group.github.io/SinNeRF/">SinNeRF </a> (ECCV'22), <a href="https://vita-group.github.io/NeuralLift-360/">NeuralLift-360 </a> (CVPR'23, Highlight). </li>
                                <li> <a href="https://zehaozhu.github.io/FSGS/">FSGS </a> (ECCV'24), <a href="https://dreamscene360.github.io/">DreamScene360 </a> (ECCV'24). </li>
                                <li> <a href="https://4k4dgen.github.io/">4K4DGen </a> (ICLR'25) </li>
                            </ul>

                                </div>
                    </div>
                </div>


                <div class="publication_publication__1Icb_">
                    <div class="publication_image__1EUuC"><img src="./Homepage_files/rs_is+lg.svg"
                        alt="loading...">
                </div>
                    <div class="publication_info__kLRGP">
                        <div class="publication_title__3m6SE">Ultra-Efficient 3D Reconstruction and Rendering</div>
                        <div class="publication_authors__qkFXc">Current modular approaches for pose estimation and dense reconstruction
                            are slow and prone to errors. My research introduces an end-to-end framework that optimizes 3D structures and
                            camera parameters under self-supervision, reducing training time from hours to seconds.
                            It can also simulate high-quality lunar surface for astronomical exploration with generative texture synthesis.
                            It also improves rendering
                            speed by 60% and compresses 3D models by over 15 times, enabling scalable, high-performance 3D systems.
                        </div>
                        <div class="publication_links__aEpO_">
                            <ul>
                                <li> <a href="https://lightgaussian.github.io/">LightGaussian </a> (NeurIPS'24, Spotlight) </li>
                                <li> <a href="https://anonymousi079j.github.io/moonsim/">MoonSim </a> (submitted) </li>
                                <li> <a href="https://instantsplat.github.io/">InstantSplat </a>(Preprint), VideoLifter (submitted).</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="publication_publication__1Icb_">
                    <div class="publication_image__1EUuC"><img src="./Homepage_files/rs_lsm.svg"
                        alt="loading...">
                </div>
                    <div class="publication_info__kLRGP">
                        <div class="publication_title__3m6SE">Semantic 3D Foundation Model: Reconstruction, Understanding and Beyond</div>
                        <div class="publication_authors__qkFXc">Humans perceive, reason, and interact with their environment through an innate understanding of structure and properties. To equip intelligent machines with similar abilities, it is vital to enable direct geometry perception from visual inputs, bypassing offline pose preprocessing for scalable and reliable planning.

                            Next-generation learning algorithms shall inherently perceive geometric structures, pre-train 3D models on Internet-scale video data, and integrate with visual-language models for reasoning and planning. Advancing architectures to process and interpret high-resolution temporal visual streams is key to enabling robust 3D understanding and interaction.
                        </div>
                        <div class="publication_links__aEpO_">
                            <ul>
                                <li> <a href="https://feature-3dgs.github.io/">Feature 3DGS </a> (CVPR'24, Highlight), <a href="https://openreview.net/forum?id=kfOtMqYJlUU">NeRF-SOS </a> (ICLR'23). </li>
                                <li> <a href="https://largespatialmodel.github.io/">Large Spatial Model </a>(NeurIPS'24).</li>
                                <li> Geometric Language Model (ongoing).</li>
                            </ul>
                        </div>
                    </div>
                </div>


                <div>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__full_size"><video src="./Homepage_files/videos/homepage_applications.mp4"
                            title="Video demos for application is loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video>
                    </div>
                </div>
                <p>My research has been demonstrated on platforms such as Quest 3, implemented within IARPA projects, and integrated into multiple commercial products.</p>




            </section>

            <section>
                <h2>Selected Publications<br class="publication-list_mobileBreak__24vsO"><span
                        class="publication-list_filters__3ikvu"><span>Full publication list at </span><span><a
                                href="https://scholar.google.com.hk/citations?hl=en&user=tdoBO3UAAAAJ&view_op=list_works&sortby=pubdate">Google
                                Scholar</a></span></span></h2>
                <div class="publication-list_smallText__pUJXB">* denotes equal contribution, &dagger; denotes project lead.</div>
                <div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/lsm.mp4"
                            title="Large Spatial Model video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Large Spatial Model: Real-time Unposed Images to Semantic 3D</div>
                            <div class="publication_authors__qkFXc"><span>Zhiwen Fan* <sup>&dagger;</sup>, Jian Zhang*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi,  Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://largespatialmodel.github.io/">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://largespatialmodel.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://largespatialmodel.github.io/">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/garden_lighgaussian.mp4"
                            title="LightGaussian video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan*<sup>&dagger;</sup></span></span><span><span>Kevin</span>
                                    <span>Wang*</span></span><span><span>Kairun </span>
                                    <span>Wen</span></span><span><span>Zehao </span>
                                    <span>Zhu</span></span><span><span>Dejia </span>
                                    <span>Xu</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span class="publication_highlights__2ILmf">(Spotlight, top 2.1% among 15671)</span>&nbsp;&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2311.17245">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://lightgaussian.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/LightGaussian">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/moonsim.mp4"
                            title="Moonsim video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">MoonSim: A Photorealistic Lunar Environment Simulator </div>
                            <div class="publication_authors__qkFXc"><span>Ziyu Chen*, Henghui Bao*, Ting-Hsuan Chen*, Haozhe Lou, Ge Yang, Zhiwen Fan, Marco Pavone, Yue Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>under submission</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://anonymousi079j.github.io/moonsim/">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://anonymousi079j.github.io/moonsim/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://anonymousi079j.github.io/moonsim/">Code</a></span></div>
                        </div>
                    </div>



                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/eva.mp4"
                            title="Large Spatial Model video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Expressive Gaussian Human Avatars from Monocular RGB Video</div>
                            <div class="publication_authors__qkFXc"><span>Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2407.03204">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://evahuman.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://evahuman.github.io/">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/mmgsslam.mp4"
                            title="MM3DGS-SLAM video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements </div>
                            <div class="publication_authors__qkFXc"><span>Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu </span></div>
                            <div class="publication_venue__1Dv6R"><span>IROS 2024 <span class="publication_highlights__2ILmf">(Oral Pitch Highlight)</span></span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2404.00923">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://vita-group.github.io/MM3DGS-SLAM/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/MM3DGS-SLAM">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/4k4dgen.mp4"
                            title="4K4DGen video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">4K4DGen: Panoramic 4D Generation at 4K Resolution</div>
                            <div class="publication_authors__qkFXc"><span>Renjie Li, Panwang Pan, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, Zhiwen Fan<sup>&dagger;</sup> </span></div>
                            <div class="publication_venue__1Dv6R"><span>ICLR 2025</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://4k4dgen.github.io/">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://4k4dgen.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://4k4dgen.github.io/">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/instantsplat.mp4"
                            title="Instantsplat video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">InstantSplat: Sparse-view Pose-free Gaussian Splatting in Seconds</div>
                            <div class="publication_authors__qkFXc"><span>Zhiwen Fan*<sup>&dagger;</sup>, Wenyan Cong*, Kairun Wen*, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>Preprint</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2403.20309">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://instantsplat.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/NVlabs/InstantSplat">Code</a></span></div>
                        </div>
                    </div>



                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/dreamscene360.mp4"
                            title="dreamscene360 video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</div>
                            <div class="publication_authors__qkFXc"><span>Shijie Zhou*, Zhiwen Fan*, Dejia Xu*, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi </span></div>
                            <div class="publication_venue__1Dv6R"><span>ECCV 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2404.06903">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://dreamscene360.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://dreamscene360.github.io/">Code</a></span></div>
                        </div>
                    </div>

                    <!-- <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/fsgs.mp4"
                            title="FSGS video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan*</span></span><span><span>Zehao</span>
                                    <span>Zhu*</span></span><span><span>Yifan</span>
                                    <span>Jiang</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>Preprint</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2312.00451">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zehaozhu.github.io/FSGS/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/FSGS">Code</a></span></div>
                        </div>
                    </div> -->

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/feature-3dgs.mp4"
                            title="Feature 3DGS video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</div>
                            <div class="publication_authors__qkFXc">
                                    Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi,</span>
                                    </div>
                            <div class="publication_venue__1Dv6R"><span>CVPR 2024</span><span class="publication_highlights__2ILmf">(Highlight, 2.8% of 11532)</span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2312.03203">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://feature-3dgs.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/ShijieZhou-UCLA/feature-3dgs">Code</a></span></div>
                        </div>
                    </div>


                    <!-- <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/tamingtext23d.mp4"
                            title="Taming... video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Taming Mode Collapse in Score Distillation for Text-to-3D Generation</div>
                            <div class="publication_authors__qkFXc">
                                    Peihao Wang, Dejia Xu, Zhiwen Fan, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra,</span>
                                    </div>
                            <div class="publication_venue__1Dv6R"><span>CVPR 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2401.00909">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://vita-group.github.io/3D-Mode-Collapse/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/3D-Mode-Collapse">Code</a></span></div>
                        </div>
                    </div> -->

                    <!-- <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/pf-grt.mp4"
                            title="PF-GRT video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Pose-Free Generalizable Rendering Transformer</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan*</span></span><span><span>Panwang</span>
                                    <span>Pan*</span></span><span><span>Peihao</span>
                                    <span>Wang</span></span><span><span>Yifan</span>
                                    <span>Jiang</span></span><span><span>Hanwen </span>
                                    <span>Jiang</span></span><span><span>Dejia </span>
                                    <span>Xu</span></span><span><span>Zehao </span>
                                    <span>Zhu</span></span><span><span>Dilin </span>
                                    <span>Wang</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>Preprint</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2310.03704">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zhiwenfan.github.io/PF-GRT/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/zhiwenfan/PF-GRT">Code</a></span></div>
                        </div>
                    </div> -->

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/nerf_sos.mp4"
                            title="NeRF-SOS video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">NeRF-SOS: Any-View Self-supervised Object Segmentation from Complex Real-World Scenes</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan</span></span><span><span>Peihao</span>
                                    <span>Wang</span></span><span><span>Yifan</span>
                                    <span>Jiang</span></span><span><span>Xinyu </span>
                                    <span>Gong</span></span><span><span>Dejia </span>
                                    <span>Xu</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>ICLR</span><span>2023</span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openreview.net/pdf?id=kfOtMqYJlUU">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zhiwenfan.github.io/NeRF-SOS/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/NeRF-SOS">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/neuralift360_merge.mp4"
                            title="NeuralLift-360 video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views</div>
                            <div class="publication_authors__qkFXc"><span><span>Dejia</span>
                                    <span>Xu</span></span><span><span>Yifan</span>
                                    <span>Jiang</span></span><span><span>Peihao</span>
                                    <span>Wang</span></span><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Yi </span>
                                    <span>Wang</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>CVPR</span><span>2023</span>&nbsp;  <span
                                class="publication_highlights__2ILmf">(Highlight, 2.5% of 9155)</span></div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2211.16431">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://vita-group.github.io/NeuralLift-360/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/NeuralLift-360">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/INS_merge.mp4"
                                title="INS video loading.." playsinline="" autoplay="" loop="" preload=""
                                muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Unified Implicit Neural Stylization</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan*<sup>&dagger;</sup></span></span><span><span>Yifan</span>
                                    <span>Jiang*</span></span><span><span>Peihao</span>
                                    <span>Wang*</span></span><span><span>Xinyu </span>
                                    <span>Gong</span></span><span><span>Dejia</span>
                                    <span>Xu</span></span><span><span>Zhangyang</span> <span>Wang</span></span></div>
                            <div class="publication_venue__1Dv6R"><span>ECCV</span><span>2022</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2204.01943">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zhiwenfan.github.io/INS/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/INS">Code</a></span></div>
                        </div>
                    </div>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/m3vit.png"
                                alt="M^3ViT Video thumbnail loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">M^3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen </span>
                                    <span>Fan*<sup>&dagger;</sup></span></span><span><span>Hanxue </span>
                                    <span>Liang*</span></span><span><span>Rishov </span>
                                    <span>Sarkar</span></span><span><span>Ziyu </span>
                                    <span>Jiang</span></span><span><span>Tianlong </span>
                                    <span>Chen</span></span><span><span>Kai </span>
                                    <span>Zou</span></span><span><span>Yu </span>
                                    <span>Cheng</span></span><span><span>Cong  </span>
                                    <span>Hao</span></span><span><span>Zhangyang   </span>
                                    <span>Wang</span>
                                </div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS</span><span>2022</span>&nbsp;
                                <span
                                class="publication_highlights__2ILmf">(QIF 2022 Award & DAC 3rd best demo)</span>
                            </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2210.14793">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/M3ViT">Code</a></span></div>
                        </div>
                    </div>
                    <!-- <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/sinnerf_merge.mp4"
                            title="INS video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image
                            </div>
                            <div class="publication_authors__qkFXc"><span><span>Dejia </span>
                                    <span>Xu*</span></span><span><span>Yifan</span>
                                    <span>Jiang*</span></span><span><span>Peihao </span>
                                    <span>Wang</span></span><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Humphery </span>
                                    <span>Shi</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>ECCV</span><span>2022</span>&nbsp;
                            </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2204.00928">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://vita-group.github.io/SinNeRF/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/SinNeRF">Code</a></span></div>
                        </div>
                    </div> -->
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/floorplancad.png"
                            alt="CADTransformer...">
                    </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Tianlong </span>
                                    <span>Chen</span></span><span><span>Peihao </span>
                                    <span>Wang</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>CVPR</span><span>2022</span>&nbsp;<span
                                class="publication_highlights__2ILmf">(Oral Presentation, top 5% in all submissions)</span> </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.html">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/CADTransformer">Code</a></span></div>
                        </div>
                    </div>

                    <!-- <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/floorplancad.png"
                            alt="FloorPlanCAD...">
                    </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol Spotting</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen </span>
                                    <span>Fan*</span></span><span><span>Lingjie  </span>
                                    <span>Zhu*</span></span><span><span>Honghua </span>
                                    <span>Li</span></span><span><span>Xiaohao </span>
                                    <span>Chen</span></span><span><span>Siyu   </span>
                                    <span>Zhu</span></span><span><span>Ping   </span>
                                    <span>Tan</span></div>
                            <div class="publication_venue__1Dv6R"><span>ICCV</span><span>2021</span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2105.07147">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zhiwenfan.github.io/images/floorplancad_poster.pdf">Poster</a></span>
                                    <span
                                    class="publication_link__fXY43"><a
                                        href="https://www.youtube.com/watch?v=rcJiRQqDKbo">Video</a></span></div>
                        </div>
                    </div> -->


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/cascadecostvolume.jpeg"
                            alt="CascadeCostVolume...">
                    </div>
                    <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</div>
                            <div class="publication_authors__qkFXc">
                                  <span>Zhiwen Fan*</span>
                                  <span>Xiaodong Gu*</span>
                                  <span>Siyu Zhu</span>
                                  <span>Zuozhuo Dai</span>
                                  <span>Feitong Tan</span>
                                  <span>Ping Tan</span>
                                  </div>
                            <div class="publication_venue__1Dv6R"><span>CVPR</span><span>2020</span>&nbsp;<span
                                class="publication_highlights__2ILmf">(Oral Presentation, top 5% in all submissions)</span> </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/1912.06378">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://www.youtube.com/watch?v=rcJiRQqDKbo">Video</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/alibaba/cascade-stereo">Code</a></span>
                            </div>
                        </div>
                    </div>

                    <!-- <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/disn-tip.png"
                            alt="CascadeCostVolume...">
                    </div>
                    <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">A Deep Information Sharing Network for Multi-contrast Compressed Sensing MRI Reconstruction</div>
                            <div class="publication_authors__qkFXc"><span><span>Liyan</span>
                                    <span>Sun</span></span><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Xueyang  </span>
                                    <span>Fu</span></span><span><span>Xinghao  </span>
                                    <span>Ding</span></span><span><span>Yue </span>
                                    <span>Huang</span></span><span><span>John    </span>
                                    <span>Paisley</span></div>
                            <div class="publication_venue__1Dv6R"><span>IEEE Transactions on Image Processing</span><span>2020</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/1805.02165">Paper</a></span>
                            </div>
                        </div>
                    </div> -->



                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/SegMRI-ipmi.jpeg"
                            alt="CascadeCostVolume...">
                    </div>
                    <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Joint CS-MRI Reconstruction and Segmentation with a Unified Deep Network</div>
                            <div class="publication_authors__qkFXc">
                            <span>Liyan Sun*</span>
                            <span>Zhiwen Fan*</span>
                            <span>Xinghao Ding</span>
                            <span>Yue Huang</span>
                            <span>John Paisley</span></div>
                            <div class="publication_venue__1Dv6R"><span>IPMI</span><span>2019</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/1805.02165">Paper</a></span>
                            </div>
                        </div>
                    </div>


                </div>
            </section>
            <section>
                <h2>Invited Talks</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Scalable 3D/4D Assets Creation @ <strong>Duke</strong>. November 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            E cient 3D Learning for Autonomous System @ <strong>UNC, Guest Lecture</strong>. November 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Empowering Machines to Understand 3D @ <strong>Stanford, ASU, JHU, Yale</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            3D Computer Vision @ <strong>TAMU, Guest Lecture</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            From Efficient 3D Learning to 3D Foundation Models @ <strong>UCLA and CalTech</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Towards Universal, Real-Time 3D Construction and Interaction @ <strong>TAMU AI Lunch</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Spatial Intelligence via Reconstruction, Distillation, and Generation @ <strong>Shanghai AI Lab</strong>. July 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research, <a
                                href="https://valser.org/article-761-1.html">VALSE Webinar </a></strong>. May 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research</strong>. May 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Real-Time Few-shot View Synthesis w/ Gaussian Splatting @ <strong>IARPA WRIVA Workshop</strong>. April 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Data-efficient and Rendering-efficient Neural Rendering @ <strong>IFML Workshop on Gen AI</strong>. November 2023</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Unified Implicit Neural Stylization @ <strong>Xiamen University; Kungfu.ai.</strong> July 2022</span>.
                    </li>


                </ul>
            </section>
            <section>
                <h2>Experience</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Meta, Reality Lab, Burlingame<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            NVIDIA Research (remote)<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Google AR, San Francisco<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2022)</span>.
                    </li>
                    <li>
                        <div style="display:inline">
                            	Alibaba Group, Hangzhou<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Senior Algorithm Engineer(2019 - 2021)</span>.
                    </li>
                </ul>
            </section>
            <section>
                <h2>Services</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Journal Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">TPAMI, TIP, IJCV, Neurocomputing</span>.
                    </li>
                    <li>
                        <div style="display:inline">
                            Conference  Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">NeurIPS 22/23, ICML 22/23, CVPR 22/23, ICCV 21/23, AAAI 21, ICME 2019</span>.
                    </li>
                </ul>
            </section>
            <footer class="styles_footer__3qp3V">
                <p>Last updated on <time datetime="Sep 10 2023">Sep 18, 2023</time></p>
                <p> The website template was originally borrowed from <a href="https://www.xzhou.me/">[1] </a>and <a href="https://yifanjiang19.github.io/">[2]</a>, thanks!.</p>
            </footer>
        </div>
    </div>

</body>

</html>